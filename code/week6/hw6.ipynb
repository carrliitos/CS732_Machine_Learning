{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7552144-01c4-4946-96a8-298b9c8c4137",
   "metadata": {},
   "source": [
    "# Hw 3: Nearest Neighbors\n",
    "## Submit to Dropbox Hw3 by Oct 21 \n",
    "\n",
    "### Part 1. \n",
    "\n",
    "Your programs should read files provided. In this format, each instance is described on a single line. The feature values are separated by\n",
    "commas, and the last value on each line is the class label (for classification).  Lines starting with '%' are comments\n",
    "\n",
    "- Your programs should  implement a k-nearest neighbor learner in a function according to the following guidelines:\n",
    "  - Assume that for classification tasks, the class attribute is named 'class' and it is the last attribute listed among all the attributes.\n",
    "  - Assume that all features will be numeric.\n",
    "  - Use Euclidean distance to compute distances between instances.\n",
    "  - Implement basic k-NN.\n",
    "  - If there is a tie among multiple instances to be in the k-nearest neighbors, the tie should be broken in favor of those instances that come first in the data file.\n",
    "  - If there is a tie in the class predicted by the k-nearest neighbors, then among the classes that have the same number of votes, the tie should be broken in favor of the class comes first in the data file.\n",
    "- You should include a function myKNN and should accept three arguments as follows:\n",
    "  - myKNN(traindata,testdata, k)\n",
    "  - The myKNN function should use the training set and the given value of k to make classifications/predictions for every instance in the test set. This can be called from a main calling function.\n",
    "  - The main program should  use  p-fold cross validation (set p =10) with just the training data to select the value of k (used by NN) to use for the test set by evaluating k1 k2 k3…. (set it to any values you like) and selecting the one that results in the minimal cross-validated error within the training set.\n",
    "  - To measure error, you should use mean absolute error. The following link shows how to use cross validation with python, including generating indices for each fold.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "- As output, your programs should print the value of k used for the test set on the first line, and then the predictions for the test-set instances.\n",
    "- For each instance in the test set, your program should print one line of output with spaces separating the fields.\n",
    "- For a classification task, each output line should list the predicted class label, and actual class label.\n",
    "- This should be followed by a line listing the number of correctly classified test instances, and the total number of instances in the test set.\n",
    "- This should be followed by a line listing the mean absolute error for the test instances, and the total number of instances in the test set.\n",
    "- Copy and paste this output to the .docx file you will submit to canvas.\n",
    "\n",
    "You should test your code on the following two data sets:\n",
    "- yeast_train.txt\n",
    "- yeast_test.txt\n",
    "\n",
    "### Part 2.\n",
    "\n",
    "For this part you will explore the effect of the k parameter on predictive accuracy.\n",
    "\n",
    "- For the yeast data set, draw a plot showing how test-set accuracy varies as a function of k. Your plot should show accuracy for k = 1, 5, 10, 15, 20 after p-fold cross validation (where p=10).\n",
    "- For the yeast data set, construct confusion matrices for the k = 1 and k = 15 test-set results (you don’t need to do cross validation for this). Show these confusion matrices and briefly discuss what the matrices tell you about the effect of k on the misclassifications. See how to create confusion matrices here.\n",
    "\n",
    "http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "\n",
    "The python code for confusion matrices can be found at \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "Put these results in the .docx file (from both parts) and submit to dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7dcc4c9-e940-460b-a33f-6f419ddc2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import scipy.stats as sp\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0179f2ee-761c-42a6-84db-95487eb2b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(Data):\n",
    "    n,m = Data.shape\n",
    "    print(n,m)\n",
    "    avg = np.mean(Data, axis=0);\n",
    "    for i in range(0,m - 1):\n",
    "        temp = Data[:,i] - avg[i]\n",
    "        s = np.std(Data[:, i])\n",
    "        Data[:, i] = temp / s\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d40897-2801-42b9-8e3b-92e6d54702f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myKNN(train,labeltrain,test,k):\n",
    "    n1, m1 = train.shape\n",
    "    n2, m2 = test.shape\n",
    "    D = euclidean_distances(train,test)\n",
    "    D = D.transpose()\n",
    "    Ytest = np.zeros(n2)\n",
    "    for i in range(0,n2):\n",
    "        ind = np.argsort(D[i,:])\n",
    "        ktoplabels = labeltrain[ind[0:k]]\n",
    "        L = sp.mode(ktoplabels)\n",
    "        Ytest[i] = L.mode.item()\n",
    "    return Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc2ee75-0635-4b7a-b74a-6616df573e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/yeast_train.txt\", \"r\") as yeast_in:\n",
    "    yeast_train_df = pd.read_csv(yeast_in, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83039c51-07ab-4982-a67e-e6ed4372ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = yeast_train_df[8].to_numpy()\n",
    "train = yeast_train_df.drop(yeast_train_df.columns[8], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1c5e727-7486-4e4a-ac58-3f9c8e1494f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/yeast_test.txt\", \"r\") as yeast_in:\n",
    "    yeast_test_df = pd.read_csv(yeast_in, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05013094-8971-4d02-a7de-71eb7cf7a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = yeast_test_df[8].to_numpy()\n",
    "test = yeast_test_df.drop(yeast_test_df.columns[8], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b347ffba-5c29-4abc-a87b-375a1745332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484 8\n"
     ]
    }
   ],
   "source": [
    "n1, m1 = train.shape\n",
    "n2, m2 = test.shape\n",
    "\n",
    "data = np.concatenate((train, test), axis=0)\n",
    "data = normalize(data)\n",
    "\n",
    "train = data[0:n1, :]\n",
    "test = data[n1:n1+n2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e214dfd1-eb6e-47db-a714-9d5911690e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1039, 8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a1eb49e-dc80-42d6-85ed-1a88b90d9b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07197441, -0.80667199,  0.69212358, ...,  6.50962832,\n",
       "        -0.17109621,  0.22      ],\n",
       "       [-0.72946507, -0.64522881,  0.80754232, ..., -0.0991314 ,\n",
       "         0.52121948,  0.25      ],\n",
       "       [ 0.58198136, -0.24162084,  0.46128609, ..., -0.0991314 ,\n",
       "         0.17506163,  0.26      ],\n",
       "       ...,\n",
       "       [-0.87518135, -0.24162084, -0.3466451 , ..., -0.0991314 ,\n",
       "        -1.03649083,  0.26      ],\n",
       "       [ 0.07197441, -0.40306402,  1.38463603, ..., -0.0991314 ,\n",
       "         1.55969302,  0.22      ],\n",
       "       [ 1.23770457,  0.5655951 , -1.61625127, ..., -0.0991314 ,\n",
       "         1.04045625,  0.22      ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec6a8112-439f-47c7-96f4-45d7c58d3fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29232/12476209.py:10: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  L = sp.mode(ktoplabels)\n"
     ]
    }
   ],
   "source": [
    "k = 14\n",
    "y_test = myKNN(train, label_train, test, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4787a3f9-1222-4c7e-8035-74d1306e216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21348314606741572\n"
     ]
    }
   ],
   "source": [
    "acc = ((y_test==label_test).astype('uint8')).sum()/n2\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7ee2ca6-97bb-4e4c-b649-1aac542a5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29232/12476209.py:10: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  L = sp.mode(ktoplabels)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (445,) (104,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     Ytest \u001b[38;5;241m=\u001b[39m myKNN(train[train_index,:], label_train[train_index], train[test_index], K) \u001b[38;5;66;03m#return value is the prediction\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     n \u001b[38;5;241m=\u001b[39m test_index\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43my_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# acc = ((y_test == label_train[test_index],).astype('uint8')).sum()/n[0]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# AvgAcc across each fold for a particular choice of K\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# bestK=find the K with maximum Avgacc\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (445,) (104,) "
     ]
    }
   ],
   "source": [
    "# for homework\n",
    "Kchoices=np.array([5, 10 , 15, 20])\n",
    "for i in range(0, len(Kchoices)):\n",
    "    K = Kchoices[i]\n",
    "    kf = KFold(n_splits=10)\n",
    "    kf.get_n_splits(train)\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    Ytest = myKNN(train[train_index,:], label_train[train_index], train[test_index,:], K) #return value is the prediction\n",
    "    n = test_index.shape\n",
    "    print(y_test == label_train[test_index])\n",
    "    # acc = ((y_test == label_train[test_index],).astype('uint8')).sum()/n[0]\n",
    "# AvgAcc across each fold for a particular choice of K\n",
    "# bestK=find the K with maximum Avgacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7e5fd-a665-4a87-a969-c954c93870d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
